{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abfb2a24-05ab-4b1d-8fc2-7448458663bd",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "<img src=goodplace2.png align=right width=500>\n",
    "\n",
    "The TV show *The Good Place* is centered around a number of humans who have died and find themselves in the afterlife.  In this conception\n",
    "of the afterlife, humans are sent to \"the Good Place\" or \"the Bad Place\" after death.  All humans are assigned a numerical score based on the morality of their conduct in life, and only those with the very highest scores are sent to the \"Good Place\", where they enjoy eternal happiness; all others experience an eternity of torture in the \"Bad Place.\"\n",
    "\n",
    "In this project, you will explore using logistic regression to predict whether someone will end up in the \"Good Place or the \"Bad Place\" based on an\n",
    "extremely scaled down version of their conduct in life.  In particular, we have data for 1000 people about how often they:\n",
    "\n",
    "- Let someone merge in front of them in traffic\n",
    "- Didn't tip their server at a restaurant\n",
    "- Held a door open for someone who was walking behind them\n",
    "- Littered\n",
    "\n",
    "These will be our four features for the problem.  Our data set consists of these four features tallied for 1000 different people.\n",
    "\n",
    "To complete this project, you will write Python code in places marked\n",
    "`# YOUR CODE HERE`.  There are also code cells in this notebook you must run\n",
    "to produce various kinds of plots and graphs.  There are also a number of cells\n",
    "marked with `# YOUR ANSWER HERE` where you will answer questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a37aed2-9e9c-43c6-a7fb-0d93fddff848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10cb214-0b0d-4fbc-b6e9-38a4b4e066e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letMerge</th>\n",
       "      <th>noTip</th>\n",
       "      <th>heldDoor</th>\n",
       "      <th>littered</th>\n",
       "      <th>goodbad</th>\n",
       "      <th>noisygoodbad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>7</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>295</td>\n",
       "      <td>224</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>356</td>\n",
       "      <td>182</td>\n",
       "      <td>bad</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>20</td>\n",
       "      <td>485</td>\n",
       "      <td>193</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196</td>\n",
       "      <td>24</td>\n",
       "      <td>376</td>\n",
       "      <td>175</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>137</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>439</td>\n",
       "      <td>234</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>67</td>\n",
       "      <td>22</td>\n",
       "      <td>480</td>\n",
       "      <td>113</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>470</td>\n",
       "      <td>54</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>141</td>\n",
       "      <td>4</td>\n",
       "      <td>447</td>\n",
       "      <td>74</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     letMerge  noTip  heldDoor  littered goodbad noisygoodbad\n",
       "0          13      0       102         7    good         good\n",
       "1          24     40       295       224     bad          bad\n",
       "2          42      8       356       182     bad         good\n",
       "3         194     20       485       193    good         good\n",
       "4         196     24       376       175    good         good\n",
       "..        ...    ...       ...       ...     ...          ...\n",
       "995        37      3       150       137     bad          bad\n",
       "996        15     17       439       234     bad          bad\n",
       "997        67     22       480       113    good         good\n",
       "998        57     20       470        54    good         good\n",
       "999       141      4       447        74    good         good\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "# Write code below to read the CSV file \"data1.csv\" and put it into a\n",
    "# Pandas dataframe called `df`:\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "df = pd.read_csv(\"data1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7ce96-70c8-4ae6-aecf-2d06692fe2e8",
   "metadata": {},
   "source": [
    "## Explanation of the data file\n",
    "\n",
    "Each row of the file represents data about a person.  \n",
    "\n",
    "The first four columns should be self-explanatory: they tell how often a person did a\n",
    "certain activity (explained above).  There are two columns at the end saying whether they\n",
    "ended up in the \"Good Place\" or the \"Bad Place.\"\n",
    "\n",
    "The first of the two (`goodbad`) is calculated \"perfectly\" from a formula I came up with (that I'm keeping secret!)\n",
    "\"Perfectly\" meaning that the formula itself probably isn't perfect, but the good/bad column is calculated\n",
    "directly mathematically from the formula, based on the four features.\n",
    "\n",
    "The second of the two (`noisygoodbad`) is also calculated perfectly from the formula, but\n",
    "with some \"noise\" thrown in.  In other words, I've switched a few of the goods to bads and vice versa, to\n",
    "simulate a real-world situation (where the Good Place/Bad Place determination is based not only on this\n",
    "data, but other data as well that we don't have access to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3786e4-a047-4030-bf20-e97b083b15df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letMerge</th>\n",
       "      <th>noTip</th>\n",
       "      <th>heldDoor</th>\n",
       "      <th>littered</th>\n",
       "      <th>goodbad</th>\n",
       "      <th>noisygoodbad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>7</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>295</td>\n",
       "      <td>224</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>356</td>\n",
       "      <td>182</td>\n",
       "      <td>bad</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>20</td>\n",
       "      <td>485</td>\n",
       "      <td>193</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196</td>\n",
       "      <td>24</td>\n",
       "      <td>376</td>\n",
       "      <td>175</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>137</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>439</td>\n",
       "      <td>234</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>67</td>\n",
       "      <td>22</td>\n",
       "      <td>480</td>\n",
       "      <td>113</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>470</td>\n",
       "      <td>54</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>141</td>\n",
       "      <td>4</td>\n",
       "      <td>447</td>\n",
       "      <td>74</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     letMerge  noTip  heldDoor  littered goodbad noisygoodbad\n",
       "0          13      0       102         7    good         good\n",
       "1          24     40       295       224     bad          bad\n",
       "2          42      8       356       182     bad         good\n",
       "3         194     20       485       193    good         good\n",
       "4         196     24       376       175    good         good\n",
       "..        ...    ...       ...       ...     ...          ...\n",
       "995        37      3       150       137     bad          bad\n",
       "996        15     17       439       234     bad          bad\n",
       "997        67     22       480       113    good         good\n",
       "998        57     20       470        54    good         good\n",
       "999       141      4       447        74    good         good\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few and last few lines of this data:\n",
    "\n",
    "print(len(df)) # Should be 1000\n",
    "df  # Verify this looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a4df79-02ae-47de-aef3-c381bd00a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our X and y data\n",
    "\n",
    "# First, we will split the data frame above into a four-column frame\n",
    "# with the input features (X's) and a one-column frame with the target\n",
    "# feature (y), which we will use the noisy column (noisygoodbad).\n",
    "\n",
    "# Write code below to create df_X with just the four X feature columns,\n",
    "# and df_y that has just the noisygoodbad column.\n",
    "\n",
    "# Then **normalize** the X values with Z-score normalization as in \n",
    "# project 1.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "df_X = df[['letMerge', 'noTip', 'heldDoor', 'littered']]\n",
    "\n",
    "df_y = df['noisygoodbad']\n",
    "\n",
    "\n",
    "# finding the z score \n",
    "mean = df_X.mean()\n",
    "std_dev = df_X.std()\n",
    "\n",
    "z_score = (df_X - mean)/std_dev\n",
    "df_X = z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db28502-695c-4caa-994c-d5bd1f929bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letMerge</th>\n",
       "      <th>noTip</th>\n",
       "      <th>heldDoor</th>\n",
       "      <th>littered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.161413</td>\n",
       "      <td>-1.267048</td>\n",
       "      <td>-1.350553</td>\n",
       "      <td>-1.620752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.989370</td>\n",
       "      <td>2.629464</td>\n",
       "      <td>0.136883</td>\n",
       "      <td>1.396890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.707846</td>\n",
       "      <td>-0.487746</td>\n",
       "      <td>0.607005</td>\n",
       "      <td>0.812830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.669471</td>\n",
       "      <td>0.681208</td>\n",
       "      <td>1.601198</td>\n",
       "      <td>0.965798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.700751</td>\n",
       "      <td>1.070859</td>\n",
       "      <td>0.761143</td>\n",
       "      <td>0.715487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.786047</td>\n",
       "      <td>-0.974810</td>\n",
       "      <td>-0.980621</td>\n",
       "      <td>0.187052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-1.130132</td>\n",
       "      <td>0.388969</td>\n",
       "      <td>1.246679</td>\n",
       "      <td>1.535952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.316840</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>1.562663</td>\n",
       "      <td>-0.146696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.473242</td>\n",
       "      <td>0.681208</td>\n",
       "      <td>1.485594</td>\n",
       "      <td>-0.967161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.840538</td>\n",
       "      <td>-0.877397</td>\n",
       "      <td>1.308335</td>\n",
       "      <td>-0.689037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     letMerge     noTip  heldDoor  littered\n",
       "0   -1.161413 -1.267048 -1.350553 -1.620752\n",
       "1   -0.989370  2.629464  0.136883  1.396890\n",
       "2   -0.707846 -0.487746  0.607005  0.812830\n",
       "3    1.669471  0.681208  1.601198  0.965798\n",
       "4    1.700751  1.070859  0.761143  0.715487\n",
       "..        ...       ...       ...       ...\n",
       "995 -0.786047 -0.974810 -0.980621  0.187052\n",
       "996 -1.130132  0.388969  1.246679  1.535952\n",
       "997 -0.316840  0.876033  1.562663 -0.146696\n",
       "998 -0.473242  0.681208  1.485594 -0.967161\n",
       "999  0.840538 -0.877397  1.308335 -0.689037\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check for df_X\n",
    "\n",
    "df_X  # Should print a data frame with 1000 rows and 4 columns.\n",
    "# First row should be [ -1.161413 -1.267048 -1.350553 -1.620752]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f31299c-bed6-44be-bd11-bb5eb84ba006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      good\n",
       "1       bad\n",
       "2      good\n",
       "3      good\n",
       "4      good\n",
       "       ... \n",
       "995     bad\n",
       "996     bad\n",
       "997    good\n",
       "998    good\n",
       "999    good\n",
       "Name: noisygoodbad, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check for df_y\n",
    "\n",
    "df_y  # Should be a column of goods and bads, starting with good, bad, good, good, good...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6667ab3c-7106-4b57-b612-4756034136bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "995    0\n",
       "996    0\n",
       "997    1\n",
       "998    1\n",
       "999    1\n",
       "Name: noisygoodbad, Length: 1000, dtype: int32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to switch df_y to not have good/bad strings, but rather 0's and 1's.\n",
    "# Use this line of code:\n",
    "\n",
    "df_y = (df_y == 'good').astype(int)\n",
    "\n",
    "# Sanity check: should now be a column of ones and zeros, with 1=good, 0=bad.  \n",
    "df_y   # Should begin 1, 0, 1, 1, 1, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d6faa7-5308-4884-ac49-ed5af1eb15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check shapes:\n",
    "\n",
    "print(df_X.shape) # Should be (1000, 4)\n",
    "print(df_y.shape) # Should be (1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7ea4ac9-0c3c-470b-8425-2da67ca44c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing.\n",
    "\n",
    "# We want to write code to split the data frame above into a few\n",
    "# new data frames.  In particular, are going to have a TRAINING SET\n",
    "# and a TESTING SET for this project.  We will use 80% of the data for \n",
    "# training, and the remaining 20% for testing.  \n",
    "\n",
    "# In the real world, we would split the data randomly, but so we all\n",
    "# end up with the same results, we will use the first 80% of the data\n",
    "# for training, and the last 20% for testing (in order of how the rows\n",
    "# show up in the file).  Note that there are 1000 people (rows in \n",
    "# the file), so the first 800 rows will be training, and the last 200\n",
    "# will be testing.\n",
    "\n",
    "# Write code here to create FOUR NUMPY ndarrays:\n",
    "\n",
    "# - X_train: first 800 lines of df_X\n",
    "# - X_test: last 200 lines of df_X\n",
    "# - y_train: first 800 lines of df_y\n",
    "# - y_test: last 200 lines of df_y\n",
    "\n",
    "# Then, add a column of ones to the left side of X_train and X_test.\n",
    "\n",
    "x = df_X.to_numpy()\n",
    "y = df_y.to_numpy()\n",
    "\n",
    "X_train_i = x[0:800]\n",
    "X_test_i = x[800:1000]\n",
    "y_train = y[0:800]\n",
    "y_test = y[800:1000]\n",
    "\n",
    "\n",
    "m = len(X_train_i)\n",
    "n = len(X_test_i)\n",
    "\n",
    "v1 = np.ones((m , 1))    \n",
    "v2 = np.ones((n , 1))    \n",
    "\n",
    "X_train = np.hstack((v1, X_train_i))   \n",
    "X_test = np.hstack((v2, X_test_i))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3f59aa-ba82-4ec5-9234-b390140e4800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 5)\n",
      "(800,)\n",
      "(200, 5)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks:\n",
    "\n",
    "print(X_train.shape) # Should be (800, 5)\n",
    "print(y_train.shape) # Should be (800,) \n",
    "print(X_test.shape) # Should be (200, 5)\n",
    "print(y_test.shape) # Should be (200,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129456c3-f616-41bd-a93e-79d585030e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 training examples:\n",
      "[[ 1.         -1.16141295 -1.26704849 -1.35055312 -1.62075162  1.        ]\n",
      " [ 1.         -0.98937031  2.62946412  0.13688262  1.3968899   0.        ]\n",
      " [ 1.         -0.70784599 -0.48774597  0.6070048   0.81283025  1.        ]\n",
      " [ 1.          1.66947051  0.68120782  1.6011976   0.96579826  1.        ]\n",
      " [ 1.          1.70075099  1.07085908  0.76114322  0.71548698  1.        ]\n",
      " [ 1.          0.74669634  0.77862063 -0.31782571  0.24267678  0.        ]\n",
      " [ 1.          0.60593418 -0.97481004  0.76114322 -1.28700325  1.        ]\n",
      " [ 1.         -0.86424839 -1.26704849  1.06942006 -0.78638069  1.        ]\n",
      " [ 1.          2.38892156 -0.6825716  -0.75712021 -0.49435087  1.        ]\n",
      " [ 1.         -1.36473607  1.55792315 -0.66463716 -0.52216323  0.        ]]\n",
      "\n",
      "First 10 testing examples:\n",
      "[[ 1.          0.48081226 -0.6825716   0.05210649  0.59033134  1.        ]\n",
      " [ 1.         -0.87988863 -0.29292034 -0.387188    0.0758026   0.        ]\n",
      " [ 1.          1.09078163  1.75274879 -0.80336173  0.72939316  0.        ]\n",
      " [ 1.         -0.2855595  -0.39033315 -1.35055312  0.08970878  0.        ]\n",
      " [ 1.         -0.73912647  1.07085908 -1.58946767  1.16048481  0.        ]\n",
      " [ 1.          0.8092573  -1.26704849  0.40662486 -0.99497343  1.        ]\n",
      " [ 1.          1.18462307  1.8501616  -0.84960326 -1.12012907  1.        ]\n",
      " [ 1.         -1.23961415 -0.77998441  0.54534943 -0.50825705  1.        ]\n",
      " [ 1.         -0.16043758 -0.87739723 -1.65882996  0.71548698  0.        ]\n",
      " [ 1.          0.48081226 -1.16963567 -0.70317176  1.20220335  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Show first few rows of training/testing data:  (will be useful to have these later)\n",
    "\n",
    "print(\"First 10 training examples:\")\n",
    "print(np.hstack([X_train, y_train.reshape(-1, 1)])[0:10])\n",
    "print()\n",
    "print(\"First 10 testing examples:\")\n",
    "print(np.hstack([X_test, y_test.reshape(-1, 1)])[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f575209-8473-461d-99d3-51e482549cc7",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Like in Part A of the previous project, we will rely on an external method to create\n",
    "a logistic regression model for us, then we will see if we can replicate it ourselves.\n",
    "\n",
    "Below is code that uses scikit-learn to do this for us.  Don't worry too much about what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102fdfea-65d7-4803-9ea4-c2fa5f56e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\maria\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (1.2.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\maria\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\maria\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\maria\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\maria\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "w found through scikit-learn: [ 2.17303834  6.40434357 -6.47511472  8.86631945 -9.64155641]\n"
     ]
    }
   ],
   "source": [
    "# Download and install scikit-learn if not already done:\n",
    "%pip install scikit-learn\n",
    "\n",
    "# Import the logistic regression functionality from scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model and train it on our training data:\n",
    "model = LogisticRegression(random_state=0, penalty=None, fit_intercept=False).fit(X_train, y_train)\n",
    "\n",
    "# If the line above gives an error about penalty=None, try switching that part to penalty='none' instead.\n",
    "\n",
    "# model.coef_ contains the w vector that this logistic regression model was able to find, so\n",
    "# we'll treat it as the \"best\" w and see if we can match it manually.\n",
    "w_direct = model.coef_[0]\n",
    "\n",
    "print(\"w found through scikit-learn:\", w_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cc5e005-4294-4c93-a8a7-227c6db31309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, write a sentence about how to interpret these \n",
    "# numbers in w_direct, in particular, (1) why are some negative\n",
    "# and some positive, and (2) what is the special interpretation of\n",
    "# w_direct[0]?\n",
    "\n",
    "# YOUR ANSWER HERE:\n",
    "# The w vector carries the weights of each value. Some of these weights are negative, \n",
    "# which indicates that there is a negative correlation between the weight and the x value. \n",
    "# A positive number indicates a positive correlation, and the further away the number is \n",
    "# from 0, the larger the correlation. w_direct[0] indicates a positive but small correlation \n",
    "# between the weight and the x value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031723e-d89c-4613-8eb7-a8bd7cd16b64",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "In this part you will write code for binary logistic regression by hand, including the model,\n",
    "the loss function, the cost function, and gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ea1eb0-3006-4789-a7da-50bd839697f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid function\n",
    "\n",
    "# Write code here to define the sigmoid function 1/(1+e^-x).  \n",
    "# IMPORTANT: Use the np.exp function to raise e to a power.  \n",
    "\n",
    "def sigmoid(x):\n",
    "    # 1/(1+e^-x)\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0d7adf2-20fe-4f18-a59f-863ffaf8f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.2689414213699951 0.6224593312018546\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "print(sigmoid(0), sigmoid(-1), sigmoid(0.5))\n",
    "\n",
    "# should print 0.5 0.2689414213699951 0.6224593312018546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb4f2961-7789-4ff7-8aec-35e140601a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called run_model below to run the logistic\n",
    "# regression model on one feature vector (x_data).\n",
    "# In other words, this function should compute 1/(1 + e^(-wx))\n",
    "# where x is x_data.  But do this by calling your sigmoid function\n",
    "# and the dot product function (np.dot()).\n",
    "\n",
    "def run_model(x_data, w):\n",
    "    \"\"\"\n",
    "    x_data: array of features (n+1)\n",
    "    w: array of weights (n+1)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # performs the dot product of w and x \n",
    "    # and then the sigmoid function with z\n",
    "    \n",
    "    z = np.dot(w, x_data)\n",
    "    model = sigmoid(z)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d36a583-5503-4b8b-ace8-851d3b0e5f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9986297185506662"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: run the model from Part A on the first testing example\n",
    "\n",
    "run_model(w_direct, X_train[0])  # should be 0.9986297185506662\n",
    "#print(\"1: \", run_model(w_direct, X_train[799]))\n",
    "#run_model(w_direct, X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "111adf69-bbe2-4bab-a09e-161deb3aea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# The run_model function only returns numbers in a certain range.  What is this range\n",
    "# and why does this function not return numbers outside of that range?\n",
    "\n",
    "# ANSWER:\n",
    "# The run_model function only returns numbers between 0 and 1. This is because logistic \n",
    "# regression returns 0 for false and 1 for true to determine what category something would\n",
    "# fall into. 0 would indicate that something is false whereas the closer it is to one, the \n",
    "# more likely the value is true. It cannot go over one because something cannot be more true \n",
    "# than it already is true (out of bounds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5638376f-586d-4fdc-bb96-4a1d4d322993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called make_prediction that will\n",
    "# actually predict the class 0 or 1 for a feature vector x_data.\n",
    "# To do this, just call run_model and check if the return\n",
    "# value is > or < than 0.5\n",
    "\n",
    "def make_prediction(x_data, w):\n",
    "    \"\"\"\n",
    "    x_data: array of features (n+1)\n",
    "    w: array of weights (n+1)\n",
    "    returns: 0 or 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # runs the model \n",
    "    model = run_model(x_data, w)\n",
    "    \n",
    "    # returns true if model was above .5 and false if under \n",
    "    if model >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7e4e0e9-0efc-46c1-a012-2da1a7b05ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for test example 0 -> 1\n",
      "Predicted class for test example 1 -> 0\n",
      "Predicted class for test example 2 -> 0\n",
      "Predicted class for test example 3 -> 0\n",
      "Predicted class for test example 4 -> 0\n",
      "Predicted class for test example 5 -> 1\n",
      "Predicted class for test example 6 -> 1\n",
      "Predicted class for test example 7 -> 1\n",
      "Predicted class for test example 8 -> 0\n",
      "Predicted class for test example 9 -> 0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: classify the first few testing examples using the model from Part A\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Predicted class for test example\", i, \"->\", make_prediction(X_test[i], w_direct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3309fc50-bf06-4b31-9740-6f13f1b5eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# Given the output immediately above, what is the accuracy of the model in Part A (since we used w_direct\n",
    "# above) just based on these 10 training examples?  (Answer as a percent; in other words\n",
    "# the percentage of those 10 testing examples that were predicted correctly).\n",
    "\n",
    "# ANSWER:\n",
    "# The accuracy would be 40% accurate. \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56291ae2-12bf-498b-9500-df3f1d66a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called compute_accuracy that takes a \n",
    "# set of X values and a set of y values and a parameter vector\n",
    "# w.  This function should predict the class for each example x\n",
    "# in X_data and based on the true y values (y_data), compute\n",
    "# the accuracy on this data set.\n",
    "\n",
    "# To do this, call make_prediction on each row of X_data\n",
    "# and compare the output against the corresponding value in y_data.\n",
    "# Count how many predictions are correct and divide by the total.\n",
    "\n",
    "def compute_accuracy(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix of features (flexible rows, n+1 cols)\n",
    "    y_data: vector of true classes (same number of rows as X_data)\n",
    "    w: array of weights (n+1)\n",
    "    returns: percentage of rows in X_data classified correctly\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(X_data)\n",
    "    pred_count = 0\n",
    "    correct_pred = 0\n",
    "\n",
    "    for i in range(0, m):\n",
    "        prediction = make_prediction(X_data[i], w)\n",
    "        pred_count += 1 \n",
    "        \n",
    "        if prediction == y_data[i]:\n",
    "            correct_pred += 1 \n",
    "    \n",
    "    accuracy = correct_pred / pred_count\n",
    "        \n",
    "    return accuracy \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbcece4e-4d1c-4627-ac5d-bf70f9b9b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n",
      "0.945\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "train_acc_partA = compute_accuracy(X_train, y_train, w_direct)\n",
    "test_acc_partA = compute_accuracy(X_test, y_test, w_direct)\n",
    "\n",
    "print(train_acc_partA)  # should be 0.9625\n",
    "print(test_acc_partA)  # should be 0.945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff4b3493-862b-42f9-b0cc-5dfa761729fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# Which of the two numbers above do we report as the \"true\" accuracy of our model,\n",
    "# and why do we typically not report the other (or not give it as much importance)?\n",
    "\n",
    "# ANSWER:\n",
    "# We would use the training data instead of the testing data since there is more\n",
    "# data in the training set that was used to calculate the accuracy of the algorithm\n",
    "# woth the sigmoid function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f7840f3-d429-4302-baa9-5d90c1435740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_loss function below to compute the\n",
    "# loss over *one* training example, given the true y value\n",
    "# and the predicted y value (y_hat).  \n",
    "# This is the logistic regression loss function.\n",
    "\n",
    "def compute_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    y: 0 or 1 \n",
    "    y_hat: decimal number between 0 and 1\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "        \n",
    "    if y == 1:\n",
    "        return -np.log(y_hat)             # -log(y_hat)\n",
    "    else:\n",
    "        return -np.log(1-y_hat)           # -log(1-y_hat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9d6a2e2-9c16-45fa-a359-0f83a9d3cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_cost function below to compute the\n",
    "# total cost over the entire data set X_data and y_data,\n",
    "# given parameters vector w.\n",
    "# Call your run_model() and compute_loss() functions \n",
    "# that you defined above.  You should have one loop.\n",
    "# DO NOT CALL MAKE_PREDICTION; it's not needed here.\n",
    "\n",
    "def compute_cost(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "        \n",
    "    m = len(X_data) \n",
    "    n = X_data.shape[1]\n",
    "   # print(\"m: \", m, \"n: \", n)\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    # goes through the whole matrix  \n",
    "    for i in range(0, m):\n",
    "        x = X_data[i]\n",
    "        y = y_data[i]                           # y^(i)\n",
    "        y_hat = run_model(X_data[i], w)         # y^(i) prediction \n",
    "        loss = compute_loss(y, y_hat)\n",
    "        \n",
    "        #y[i] log(y_hat) + (1-y[i]) log(1-y_hat)\n",
    "        cost = (y * loss) + ((1-y) * loss)\n",
    "        \n",
    "        total_cost += cost\n",
    "        \n",
    "    \n",
    "    return total_cost/ m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40f04248-f046-4865-9246-9d27d63afd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07352472835636173\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: compute the loss for w_direct from Part A:\n",
    "\n",
    "w_direct_cost = compute_cost(X_train, y_train, w_direct)  # This is the minimum cost we can ever get!  Should be less than 0.1.\n",
    "print(w_direct_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca88aeed-7b1a-40fc-a742-ba127c2a670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# compute_cost() above returns a single number, but based on the formula in J(w),\n",
    "# this function can only ever return numbers in a fixed range.  What is that range\n",
    "# and why are only numbers in this range ever returned?\n",
    "\n",
    "# ANSWER:\n",
    "# The range would be any number between 0 and 1. The goal of logistic regression is to\n",
    "# determine classification with a yes/no or true/false. We only use numbers between 0 and 1 \n",
    "# and the closer a number is to 0 the more likely it is false; alternatively, the closer the\n",
    "# number is to 1 the more likely it will be true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ed2c1a63-7376-41db-8235-cfee2ef8dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_gradient function below to compute\n",
    "# the complete gradient for the function J(w).  \n",
    "# Do not use matrix computations here; call your run_model() function\n",
    "# that you defined above.  You should have two nested loops.\n",
    "\n",
    "def compute_gradient(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: array of gradients (n+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    gradients = []\n",
    "    m = X_data.shape[0]\n",
    "    n = X_data.shape[1]\n",
    "    \n",
    "    # does the summation \n",
    "    for j in range(0, n):\n",
    "        gradient_w = 0                                    # summation restarts with each iteration \n",
    "        \n",
    "        for i in range(0, m):                             # computes the equation  \n",
    "            y = y_data[i]                                 # y^(i)\n",
    "            y_hat = run_model(X_data[i], w)               # y^(i) prediction \n",
    "            \n",
    "            grad_w = (y_hat - y) * X_data[i][j]\n",
    "            gradient_w += grad_w \n",
    "            \n",
    "        gradients.append(gradient_w / m)                  # puts it in the matrix \n",
    "        \n",
    "    gradients_vector = np.array(gradients)\n",
    "    \n",
    "    \n",
    "   # print(\"gradients vector: \", gradients_vector)\n",
    "    return gradients_vector\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7645ea34-7117-4971-abca-4b7084038f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final w: [ 0.24461987  0.48346015 -0.57504251  0.7290623  -0.79192554]\n"
     ]
    }
   ],
   "source": [
    "# Write code here to perform gradient descent, using your\n",
    "# functions above.  You should use three new variables in your\n",
    "# code:\n",
    "# - w_manual: which is the vector of weights that gradient\n",
    "#   descent is designed to find:\n",
    "# - w_manual_cost, which is the cost of these weights,\n",
    "# - J_list, which is the list of \n",
    "#   costs determined by compute_cost() [like in the in-class lab we did].\n",
    "\n",
    "# Setup these vars:\n",
    "m = X_train.shape[0]\n",
    "n = X_train.shape[1]\n",
    "#print(\"n: \", n)\n",
    "w_manual = np.zeros(n)  # n+1 weights\n",
    "w_manual_cost = 0\n",
    "J_list = []\n",
    "\n",
    "#ALPHA = .00000001\n",
    "ALPHA = .1\n",
    "\n",
    "\n",
    "# must run until convergence \n",
    "for ctr in range(0, 55):\n",
    "    # checks to see how each iteration changes the w and b values \n",
    "   # print(\"Iteration: \", ctr)\n",
    "   # print(\"w = \", w_manual)\n",
    "    w_manual_cost = compute_cost(X_train, y_train, w_manual)\n",
    " #   print(\"cost = \", w_manual_cost)\n",
    "    J_list.append(w_manual_cost)\n",
    "    \n",
    "    # has the derivatives after the 1/m reduction \n",
    "    gradient_w = compute_gradient(X_train, y_train, w_manual)\n",
    "    \n",
    "    # this part does the gradient descent formulas \n",
    "    w_manual = w_manual - ALPHA * gradient_w \n",
    "    \n",
    "    #print()\n",
    "    #print(\"---------------------------------------------\")\n",
    "\n",
    "# Verify:\n",
    "    \n",
    "    \n",
    "print(\"Final w:\", w_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "95163d93-73ab-4fed-8cc6-0a464f0c74cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_list)), J_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08b65c2f-022c-41b8-b9ae-a47c3e847e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep playing around with gradient descent until you have a good\n",
    "# learning curve in the plot above (something that appears to flatten out).\n",
    "# Then answer the questions below. \n",
    "\n",
    "# What was your initial choice for alpha?  Your final choice?  How did\n",
    "# you arrive at these choices?\n",
    "#\n",
    "# My alpha was .00000001 at the beginning. However, that value was too small. I kept\n",
    "# taking away 0's until the line curved with the alpha value being .1.\n",
    "#\n",
    "# How many iterations of gradient descent did you need until convergence?\n",
    "#\n",
    "# I have 55 iterations of gradient descent until it converges. \n",
    "#\n",
    "# What was your final vector of weights? (w_manual)\n",
    "#\n",
    "# [ 0.21792379  0.4237281  -0.50587933  0.64057517 -0.69671086]\n",
    "#\n",
    "# What was your final cost of these weights? (w_manual_cost)\n",
    "#\n",
    "# 0.37227474112181413\n",
    "#\n",
    "# What was your final vector of weights from Part A? (w_direct)\n",
    "#\n",
    "# [ 2.17303834  6.40434357 -6.47511472  8.86631945 -9.64155641]\n",
    "#\n",
    "# What cost of these weights? (w_direct_cost)\n",
    "#\n",
    "# 0.07352472835636173\n",
    "#\n",
    "# How close are your weights from Part B to the \"correct\" weights from Part A?\n",
    "#\n",
    "# The weights from my part B are far off from the correct weights from part A. \n",
    "#  Mine are much larger in terms of decimal place spaces. \n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b703235e-8ccc-4842-8b80-c28b1a8a38ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n",
      "0.955\n"
     ]
    }
   ],
   "source": [
    "# Write code here to compute the accuracy of the new model (the one you just trained)\n",
    "# on the training and testing data sets.  \n",
    "# Save these values to two variables called train_acc_partB and test_acc_partB.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "train_acc_partB = compute_accuracy(X_train, y_train, w_manual)\n",
    "test_acc_partB = compute_accuracy(X_test, y_test, w_manual)\n",
    "\n",
    "print(train_acc_partB)  \n",
    "print(test_acc_partB)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7f4c9d1b-8cc9-40b9-b588-4a9ca859b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# How does the accuracy of the model you created by hand in Part B compare to \n",
    "# the accuracy of the model from Part A created by scikit-learn?\n",
    "\n",
    "# ANSWER:\n",
    "#\n",
    "# The accuracy is the similar between both. While the training accuracy from part B the \n",
    "# same as the accuracy for part A. The testing accuracy is also quite close\n",
    "# and even higher in part B than part A. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bdf7cf7e-37dc-4fb5-a08b-876e1140c63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part A\n",
      "Weights: [ 2.17303834  6.40434357 -6.47511472  8.86631945 -9.64155641]\n",
      "Cost: 0.07352472835636173\n",
      "Training accuracy: 0.9625\n",
      "Testing accuracy: 0.945\n",
      "\n",
      "Part B\n",
      "Weights: [ 0.24461987  0.48346015 -0.57504251  0.7290623  -0.79192554]\n",
      "Cost: 0.34579457219552023\n",
      "Training accuracy: 0.9625\n",
      "Testing accuracy: 0.955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final checkpoint\n",
    "\n",
    "# All of these should print OK and match up with what you have above:\n",
    "\n",
    "print(\"Part A\")\n",
    "print(\"Weights:\", w_direct)\n",
    "print(\"Cost:\", w_direct_cost)\n",
    "print(\"Training accuracy:\", train_acc_partA)\n",
    "print(\"Testing accuracy:\", test_acc_partA)\n",
    "print()\n",
    "print(\"Part B\")\n",
    "print(\"Weights:\", w_manual)\n",
    "print(\"Cost:\", w_manual_cost)\n",
    "print(\"Training accuracy:\", train_acc_partB)\n",
    "print(\"Testing accuracy:\", test_acc_partB)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf88d4-1db9-48a1-bca7-44e87b9c2dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
